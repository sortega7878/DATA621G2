---
title: "Data 621 Group 2 Homework #5: Wine Sales Prediction"
author: "Omar Pineda, Jeff Littlejohn, Sergio Ortega Cruz, Chester Poon, Simon Ustoyev"
date: "Due: December 5, 2019"
output: 
  pdf_document: 
    toc: yes
geometry: margin=0.5in
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, comment=NA, fig.align='center')
knitr::opts_chunk$set(error = TRUE)

library(dplyr)
library(VIM)
library(ggplot2)
library(MASS)
library(gridExtra)
library(caret)
library(pROC)
library(grid)
library(leaps)
library(reshape2)
library(vcd)
library(glmnet)
library(e1071)
library(car)
library(pander)

train = read.csv("https://raw.githubusercontent.com/sortega7878/DATA621G2/master/HW5/wine-training-data.csv")
train <- train[-1] #remove index column

test = read.csv("https://raw.githubusercontent.com/sortega7878/DATA621G2/master/HW5/wine-evaluation-data.csv")
```

#Data Exploration

The data set contains 12,795 cases , with a variable (`INDEX`), 14 predictors, and one response variable. Each case is a commerically available wine, with the response variable being the number of cases purchased by restaurants and wine shops after sampling the wine.  Of the 14 predictor variables, 12 are related to chemical preoperties of the wine, while the other two have to do with a rating and label design. 

A summary of each variable is presented below:

\newpage

```{r summary}
means <- sapply(train, function(y) mean(y, na.rm = TRUE))
mins <- sapply(train, function(y) min(y, na.rm=TRUE))
medians <- sapply(train, function(y) median(y, na.rm = TRUE))
maxs <- sapply(train, function(y) max(y, na.rm=TRUE))
IQRs <- sapply(train, function(y) IQR(y, na.rm = TRUE))
SDs <- sapply(train, function(y) sd(y, na.rm = T))
skews <- sapply(train, function(y) skewness(y, na.rm = TRUE))
cors <- as.vector(cor(train$TARGET, train[ , 1:ncol(train)], use = "complete.obs"))
NAs <- sapply(train, function(y) sum(length(which(is.na(y)))))

datasummary <- data.frame(means, mins, medians, maxs, IQRs, SDs, skews, cors, NAs)
colnames(datasummary) <- c("MEAN", "MIN","MEDIAN", "MAX", "IQR", "STD. DEV", 
                           "SKEW", "$r_{TARGET}$", "NAs")
datasummary <- round(datasummary, 2)

pander(datasummary)
```  

There are eight variables with missing values, with the proportion of values missing ranging from slightly over 3% to roughly 9.5%; these missing values will need to either be imputed or excluded from the dataset before starting the modeling process.  

With the exception of the predictor variables `LabelAppeal`, `AcidIndex`, `STARS`, and our response variable, the remainder of the variables are continuous, and appear to have a fairly normal distribution with a small spread; many of the values are centered around the mean. Due to the size of the dataset, observations outside of 3 sd from the mean do exist. Comparing the means and median, there is very little skew in all of these predictors.   

```{r}
#Continuous Variables

cont_vars <- train %>% dplyr::select(-c(TARGET, LabelAppeal, AcidIndex, STARS))

melted <- melt(cont_vars)

ggplot(melted, aes(value)) + geom_bar(aes(fill = variable, col = variable), alpha = 0.5, show.legend = FALSE) + facet_wrap(~variable, scale="free")+ ggtitle("Distribution of all Continuous Variables \n")
```

The other variables are discrete, taking only whole number values, and are therefore binomial distributions.

```{r}
#Poisson Distributions

poisson_vars <- train %>% dplyr::select(c(TARGET, LabelAppeal, AcidIndex, STARS))

melted <- melt(poisson_vars)

ggplot(melted, aes(value)) + geom_bar(aes(fill = variable, col = variable), alpha = 0.5, show.legend = FALSE) + facet_wrap(~variable, scale="free")+ ggtitle("Distribution of Discrete Variables \n")
```

We will revisit the `STARS` variable in the next section, as over 1/4 of the cases contain NAs. It is suspected that these values could be equal to a 0 rating, rather than a missing value is important to make this differentiation before trying to automatically trying to impute any values.  

```{r box_plots, fig.height=6}


#m <- melt(train, variable.name="Predictor")

#ggplot(m, aes(Predictor, value)) + geom_boxplot(aes(fill = Predictor), alpha = 0.75, show.legend = FALSE) + facet_wrap(~Predictor, scale="free") + scale_y_continuous('') + scale_x_discrete('', breaks = NULL) + ggtitle("Distribution of Predictor and Target Variables\n")
```  

Generating boxplots and dividing them up by the response variable, the three variables that show some type of correlation to `TARGET`, `LabelAppeal`, `AcidIndex` and `STARS`, become more apparent. Clearly the effect of bottle aesthetics and ratings of the wine by experts seems to have a greater effect on the decision to purchase the wine or not than any of the chemical properties. The larget majority of wine purchased in higher case numbers (> 4 cases) have higher label likability (`LabelAppeal` is not negative).

```{r}
density_df2 <- train

melted2 <- melt(density_df2, id=1)
melted2$TARGET <- factor(melted2$TARGET)

ggplot(melted2, aes(TARGET, value)) + geom_boxplot(aes(fill = TARGET), alpha = 0.5) + facet_wrap(~variable, scales="free") + scale_fill_discrete(guide = FALSE) + scale_y_continuous('', labels = NULL, breaks = NULL) + scale_x_discrete('') + ggtitle("Distribution of Predictors by TARGET\n")
```

#Data Preparation

Before attempting to combine or transform any of our variables, the predictor variables with missing values must be addressed. Eight of the predictor variables have missing values, with almost a third of our cases containing an NA. As concluded from our examination of the data in the previous section, the `STARS` variable has over 3000 NA values, which are most likely associated with a 0 rating. Using this reasoning, we will replace the NA values in `STARS` with zeros. 

```{r stars_repl_NA, echo=FALSE}
train$STARS[is.na(train$STARS)] <- 0
```  

The majority of the zero-star ratings we have used to replace the NAs have been placed into cases where our `TARGET` variable is equal to zero. This will increase the correlation between the predictor and the response, and `STARS` will remain the variable with the highest correlation with `TARGET`. 

```{r}
#Histogram split by STARS
ggplot(train, aes(TARGET, fill = factor(STARS))) +
  geom_histogram(binwidth=1, position="dodge") + scale_fill_discrete(name = "STARS") + theme(legend.position = "bottom")
```  

The remaining variables with NAs, along with the count and proportion of the total cases is found in the table below:

Var Name | No. NAs | % of Cases
-------- | ------- | ----------  
ResidualSugar | 616 | 0.048
Chlorides | 638 | 0.05  
FreeSulfurDioxide | 647 | 0.051  
TotalSulfurDioxide | 682 | 0.053 
pH | 395 | 0.031  
Sulphates | 1210 | 0.095  
Alcohol | 653 | 0.051  

```{r row_with_NA, echo=FALSE, eval=FALSE}
row_has_NA <- apply(train, 1, function(x){any(is.na(x))})

sum(row_has_NA) #6359 rows (just about 1/3) have at least 1 NA before treating for NA
#4120 rows (nearly 1/3) have at least 1 NA after replacing NA w/0 in STARS
``` 

With the exception of `Sulphates`, the missing values are approximately 5% or less of the each of the above variables. After replacing NAs in the `STARS` variable, nearly one-third of our dataset contains a row with an NA. The plots below show the distribution of the NA values for each of the remaining predictors, which appear to be missing at random.

```{r NA_distribution}
wine_new <- train

wine_new$ResidualSugar_NA <- factor(ifelse(is.na(train$ResidualSugar), 1, 0))
wine_new$Chlorides_NA <- factor(ifelse(is.na(train$Chlorides), 1, 0))
wine_new$FreeSulfurDioxide_NA <- factor(ifelse(is.na(train$FreeSulfurDioxide), 1, 0))
wine_new$TotalSulfurDioxide_NA <- factor(ifelse(is.na(train$TotalSulfurDioxide), 1, 0))
wine_new$pH_NA <- factor(ifelse(is.na(train$pH), 1, 0))
wine_new$Sulphates_NA <- factor(ifelse(is.na(train$Sulphates), 1, 0))
wine_new$Alcohol_NA <- factor(ifelse(is.na(train$Alcohol), 1, 0))


p1 <- ggplot(wine_new, aes(x=ResidualSugar_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[16])
p2 <- ggplot(wine_new, aes(x=Chlorides_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[17])
p3 <- ggplot(wine_new, aes(x=FreeSulfurDioxide_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[18])
p4 <- ggplot(wine_new, aes(x=TotalSulfurDioxide_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[19])
p5 <- ggplot(wine_new, aes(x=pH_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[20])
p6 <- ggplot(wine_new, aes(x=Sulphates_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[21])
p7 <- ggplot(wine_new, aes(x=Alcohol_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[22])

grid.arrange(p1,p2,p3,p4,p5,p6,p7, ncol=2)
```

Even though we have such a large number of observations, deleting such a large quantity of cases may affect the prediction ability of our models. Deleting all of the cases with missing values will result in the reduction of our data set by about 1/3, but due to the size of the data set, the predictors have very little skew, and the majority of values being clustered around the mean, the cases with NA values will be removed from the data set.  Imputing the mean or median will simply add to the clustering of values around the center of each distribution. A visual comparison of the predictor variables and the quantitiy of NAs is below:  

```{r NAs_count_plot}
df_varHasNA <- train %>% 
  dplyr::select(-c(TARGET, FixedAcidity, VolatileAcidity, CitricAcid,Density, LabelAppeal, AcidIndex, STARS))

#plot of NA counts
aggr(df_varHasNA, prop = TRUE, numbers = TRUE, sortVars = TRUE, cex.lab = 0.4, cex.axis = par("cex"), cex.numbers = par("cex"))
```

```{r delete_NAs}
# delete incomplete cases
train <- data.frame(train[complete.cases(train), ])
```  

Histograms of each variable in the data set after casewise deleteion is below:

```{r}
# check adding zero values for STARS, re-run.

melted3 <- melt(train)

ggplot(melted3, aes(value)) + geom_bar(aes(fill = variable, col = variable), alpha = 0.5, show.legend = FALSE) + facet_wrap(~variable, scale="free")+ ggtitle("Density of Variables After Casewise Deletion\n")
```

Examinging all of the variables, there do not appear to be any values far outside of the ranges, values that are nonsensical (negative cases purchased, for example), or need to be removed before moving forward. The combination of variables, or ratios of existing predictors did not seem to yield any effective results.

Log or square root transformations may help to improve models using some of the slightly skewed predictors.  In particular, `AcidIndex` appears the most skewed of the predictors.  Log and square root transformations are performed to observe the change in correlation of this variable with `TARGET` --- both improve the correlation by a very small amount, but the log-transform yields a greater improvement.

\newpage

#Model Creation

Only one response variable exists, and we will use at least two versions of three different types of models. Because the `TARGET` variable is a poisson distribution, we will create two poisson regression models, followed by two models using the negative binomial regression model, and lastly at least two using our familiar multiple linear regression model.  

Before creating the different models, we will investigate using Bayesian Information Criteria (BIC) and Mallow's $C_p$ to determine the quantity of predictors, and which ones specifically to use in our models.

###BIC Predictor Selection

First, we will look at predictor selection using Bayesian Information Criteria:  

```{r bic_selection}
regfit.full=regsubsets(TARGET ~., data=train, nvmax = 14)
reg.summary <- summary(regfit.full)

par(mfrow=c(1,2))
plot(regfit.full, scale = "bic", main = "Predictor Variables vs. BIC")
plot(reg.summary$bic, xlab="Number of Predictors", ylab="BIC", type="l", main="Best subset Selection using BIC")
#which.min(reg.summary$bic)
points(7, reg.summary$bic[7], col="red", cex=2, pch=20)
#reg.summary$bic
```  

The first plot shows that our most significant predictor, `STARS` would appear in every model, but more effective models would contain the `AcidIndex` and `LabelAppeal` predictors as well. The plot on the right shows the lowest BIC values for models using 7 predictors. We will investigate if adding additional predictors into the model is worth giving up the simplicity of the model. The difference in BIC values for 3 vs. 7 predictors is not drastic.

###Mallow's $C_p$ Predictor Selection  

```{r mallow_C_p}
par(mfrow=c(1,2))
plot(regfit.full, scale="Cp", main="Predictor Variables vs. Cp")
plot(reg.summary$cp, xlab="Number of Predictors", ylab="Cp", type="l", main="Best subset Selection using Cp" )

#which.min(reg.summary$cp) 
points(11, reg.summary$cp[11], col="red", cex=2, pch=20)
par(mfrow=c(1,1))
```  

Using Mallow's $C_p$, the smallest Cp values are associated with models with 11 predictors. The more parsimonious models would contain the higher correlated variables (`STARS`, `LabelAppeal`, and `AcidIndex`), but the lowest Cp value model also contains `VolatileAcidity`, `Chlorides`, `FreeSulfurDioxide`, `TotalSulfurDioxide`, `Alcohol`, `Density`, `Sulphates`, and `pH`. 

##Poisson Regression

###Model 0: Full Model  

First, a full model using Poisson regression and all 14 of the predictors is built:

```{r}
model0 <- (glm(TARGET ~ ., family="poisson", data=train))
pander(summary(model0))
#AIC: 31705
```

Seven of the predictor coeffcients are significant under a reasonable $\alpha$ of 0.05, and one more, `pH` being just over the threshold. The coefficients themselves are fairly small, with most of the more significant predictors having slopes of greater magnitude than those which are not. Eight of the coefficients have negative slopes, while the other six are positive. Those with positive coefficients, such as `STARS` and `LabelAppeal`, make sense as greater ratings and more appealing label aesthetics would entice buyers to purchase more wine.  

###Model 1: Reduced Model  

A reduced model is created, using the predictors recommended by using Mallow's $C_p$ to determine the best fit. Here, most of the predictors are the highly significant ones from the full poisson regression model.

```{r}
model1 <- glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + TotalSulfurDioxide + LabelAppeal + AcidIndex + STARS + Alcohol + Density + Sulphates + pH, family="poisson", data=train)
pander(summary(model1))
#AIC: 31700
```  

Of the 11 predictors, five have positive coefficients, six have negative coefficients. Increases in acidity and chlorides seem to have a negative effect on the number of cases purchased; this may have to do with any wines with more extreme values in these predictors to demand a specific pallette.  Four variables are not significate at the $\alpha = 0.5$ level: `Alcohol`, `Density`, `Sulphates`, and `pH`.  The predictors most correlated to `TARGET` have the most significant statistical significance. The reduced model using the recommended predictors from Mallow's $C_p$ only returns a very slight reduction in AIC.

###Model 2: Significantly Reduced Model 

For our last poisson regression model, the predictors which were the most significant and highly correlated to our response variable are used:

```{r}
model2 <- (glm(TARGET ~ LabelAppeal + AcidIndex + STARS, family=poisson, data=train))
pander(summary(model2))
#AIC: 31739
```  

Here the AIC value increases again slightly, but only three predictors out of 14 are used. The difference in goodness of fit may not be enough to justify using a simpler model. This will be investigated further in the model selection section.  There is also evidence of overdispersion, given that the residual deviance divided by the df is > 1.  The overdispersion may be the result of outliers, possibly in the response variable, given that the largest number of cases ordered happens less frequently than for the other amounts.

##Negative Binomial Regression  

One way of possibly dealing with the overdispersion is using a negative binomial (NB) regression model. Negative binomial regression has the $k$ parameter, which is the dispersion parameter. As this parameter gets larger, the variance will converge to the mean.  

###Model 3: BIC Predictors

For the first negative binomial regression model, the predictors selected using BIC are used.

```{r, error=FALSE}
model3 <- glm.nb(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + TotalSulfurDioxide + AcidIndex + LabelAppeal + STARS, data=train)
#pander(summary(model3))
summary(model3)
#AIC 31704
```  

The AIC for this model is 31704, similar to what was achieved using the predictors selected by Mallow's $C_p$ for the Poisson regression. Also similar are the coefficients and directions of the slopes, which is a result of the overdispersion. 

###Model 4: Significant Predictors

For the second negative binomial model, only the predictors with highly significant coefficients from the previous model are used:

```{r, error=FALSE}
model4 <- glm.nb(TARGET ~ VolatileAcidity + TotalSulfurDioxide + LabelAppeal + AcidIndex + STARS, data=train)
#pander(summary(model4))
summary(model4)
#AIC: 31715
```

The AIC for this model increased slightly to 31715.  The coefficients for the reduced model are quite similar to the full BIC criteria model, but their significance has increased.

##Multiple Linear Regression  

In response to the high level of dispersion exhibited, two multiple linear regression models are created on the data.  A full model, including the log-transformed acid index variable, is built (but not presented here) to aid in variable selection.

###Model 5: Reduced Model

The 7 most significant predictors from the full model are used to create a modified linear model.  Most of these predictors match the BIC-selected predictors (with the exception of the log transformation on `AcidIndex`).

```{r}
model5 <-glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + TotalSulfurDioxide + LabelAppeal + log(AcidIndex) + STARS, family="gaussian", data=train)
pander(summary(model5))
#AIC: 29584
```

This model yields an AIC of 29584.  The signs of these coefficients match those of the BIC-criteria negative binomial model.  Many of the coefficients have similar magnitudes, but in this model, the effects of `LabelAppeal` and `STARS` are greater by roughly three-fold.  Additionally, the coefficient for the `AcidIndex` variable has increased roughly 20-fold, but this can be largely attributed to the fact that it refers to the transformed variable.  All predictors show very high statistical significance.

### Model 6: Significant Predictors

A second linear model is created, including any non-transformed predictors that held statistical significance at the $\alpha = 0.10$ level.

```{r}
model6 <- glm(TARGET ~ VolatileAcidity + FreeSulfurDioxide + TotalSulfurDioxide + Chlorides + Density + pH + Sulphates + LabelAppeal + AcidIndex + STARS, family=gaussian, data = train)
pander(summary(model6))
#AIC: 29570
```  

This multiple linear model returns an AIC of 29570, the lowest of any models created.  The coefficients have the same sign and very similar magnitudes as those of the reduced model above.  The three predictors not included in model 5 have similar coefficients to those of model 3 above.  These three predictors -- `Density`, `pH`, and `Sulphates` -- are not significant at the $\alpha = 0.5$ level, but two of them are marginally not significant, and all three would still be significant at the $\alpha = 0.10$ level.

#Model Selection and Prediction  

##Model Comparsion

The characteristics and performance of the seven multiple linear regression models from the previous section are compared below:

|Model #| Type | # of Predictors|AIC|
|---|---|---|---|
|0|Poisson|14|31705|
|1|Poisson|11|31700|
|2|Poisson|3|31739|
|3|Negative Binomial|7|31704|
|4|Negative Binomial|5|31750|
|5|Mult. Linear Reg.|7|29598|
|6|Mult. Linear Reg.|10|29570|

##10-fold Cross Validation  

```{r, echo = FALSE}
k = 10
set.seed(1306)
folds = sample(1:k, nrow(train), replace = TRUE)

cv.errors0 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors1 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors2 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors3 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors4 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors5 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors6 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))

#train$target = as.numeric(as.character(train$target))

for (j in 1:k) {
  model0 <- glm(TARGET ~ ., family="poisson", data=train[folds != j, ])
  model1 <- glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + 
                  TotalSulfurDioxide + LabelAppeal + AcidIndex + STARS + Alcohol + 
                  Density + Sulphates + pH, family="poisson", data=train[folds != j, ])
  model2 <- glm(TARGET ~ LabelAppeal + AcidIndex + STARS, family="poisson", data=train[folds != j, ])
  model3 <- glm.nb(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + 
                      TotalSulfurDioxide + Alcohol + AcidIndex + LabelAppeal + STARS, data=train[folds != j, ])
  model4 <- glm.nb(TARGET ~ VolatileAcidity + TotalSulfurDioxide + LabelAppeal +
                      AcidIndex + STARS, data=train[folds != j, ])
  model5 <- glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + 
                  TotalSulfurDioxide + LabelAppeal + log(AcidIndex) + STARS, family="gaussian", data=train[folds != j, ])
  model6 <- glm(TARGET ~ VolatileAcidity + FreeSulfurDioxide + TotalSulfurDioxide + 
                  Chlorides + Density + pH + Sulphates + LabelAppeal + AcidIndex + STARS, family="gaussian", data = train[folds != j, ])
  
  
    #best.fit = regsubsets(y ~ ., data = train_df[folds != j, ], nvmax = 10)
    for (i in 1:10) {
      f = train[folds == j, ]
      #f = f[complete.cases(f),]
      
      pred0 = predict(model0, f, id = i)
      cv.errors0[j, i] = mean((train$TARGET[folds == j] - pred0) ^ 2, na.rm = TRUE)
      
      pred1 = predict(model1, f, id = i)
      cv.errors1[j, i] = mean((train$TARGET[folds == j] - pred1) ^ 2, na.rm = TRUE)

      pred2 = predict(model2, f, id = i) 
      cv.errors2[j, i] = mean((train$TARGET[folds == j] - pred2) ^ 2, na.rm = TRUE)

      pred3 = predict(model3, f, id = i) 
      cv.errors3[j, i] = mean((train$TARGET[folds == j] - pred3) ^ 2, na.rm = TRUE)
       
      pred4 = predict(model4, f, id = i) 
      cv.errors4[j, i] = mean((train$TARGET[folds == j] - pred4) ^ 2, na.rm = TRUE)
       
      pred5 = predict(model5, f, id = i) 
      cv.errors5[j, i] = mean((train$TARGET[folds == j] - pred5) ^ 2, na.rm = TRUE)      
       
      pred6 = predict(model6, f, id = i)
      cv.errors6[j, i] = mean((train$TARGET[folds == j] - pred6) ^ 2, na.rm = TRUE)
      
  }
  
}

mean.cv.errors0 <- apply(cv.errors0, 2, mean)
mean.cv.errors1 <- apply(cv.errors1, 2, mean)
mean.cv.errors2 <- apply(cv.errors2, 2, mean)
mean.cv.errors3 <- apply(cv.errors3, 2, mean)
mean.cv.errors4 <- apply(cv.errors4, 2, mean)
mean.cv.errors5 <- apply(cv.errors5, 2, mean)
mean.cv.errors6 <- apply(cv.errors6, 2, mean)

all.cv.error = data.frame(
mean(mean.cv.errors0),
mean(mean.cv.errors1),
mean(mean.cv.errors2),
mean(mean.cv.errors3),
mean(mean.cv.errors4), 
mean(mean.cv.errors5),
mean(mean.cv.errors6)
)

names(all.cv.error) = c("Poisson Model 0", "Poisson Model 1", "Poisson Model 2", "NB Model 3", "NB Model 4", "MLR Model 5", "MLR Model 6")
#all.cv.error
all.cv.error = t(all.cv.error)
names(all.cv.error) = c("Model", "Mean CV Error")

```  

#####Mean CV Error
```{r cv-error, echo = FALSE}
pander(all.cv.error)
```

Multiple Linear Model 6 exhibits both the lowest AIC and the lowest mean cross-validation error.  This may seem surprising given the nature of the `TARGET` data, but is a sensible outcome given the overdispersion apparent in the data.  The linear nature of this model provides the benefit of being easily understood by a wide audience as compared to Poisson or negative binomial models.

The linear model is applied to a test dataset containing response variables for 3335 cases.

```{r evaluation, echo=FALSE}
predicted_cases <- predict(model6, test, type='response')
predicted_cases_int = round(predicted_cases, 0)
```

The predicted ratings are converted to even case values by rounding to the nearest integer.  A table of the proportion of ratings at each number of cases is presented for both the test and training datasets.

```{r, echo = FALSE}
table_test <- table(predicted_cases_int) / length(predicted_cases_int)
table_test <- c(0, table_test, 0)
names(table_test)[1] <- "0"; names(table_test)[9] <- "8"
table_train <- table(train$TARGET) / length(train$TARGET)
table_ratings <- rbind(table_test, table_train)
row.names(table_ratings) <- c("Test", "Train")
pander(table_ratings, round = 3)
```

The full sets of predictions -- both raw predicted values and even case values -- for the evaluation dataset are available in Appendix A.

\newpage

# Appendix A: Index-wise Results from Predictive Model  
```{r appendix-a}
appendixA = data.frame(matrix(NA, nrow = 1667, ncol = 6))
appendixA[, 1] = test$IN[1:1667]
appendixA[, 2] = predicted_cases[1:1667]
appendixA[, 3] = predicted_cases_int[1:1667]
appendixA[, 4] = test$IN[1669:3335]
appendixA[, 5] = predicted_cases[1669:3335]
appendixA[, 6] = predicted_cases_int[1669:3335]
appendixA <- rbind(appendixA, c(test$IN[1668], predicted_cases[1668], predicted_cases_int[1688], rep(NA, 3)))

names(appendixA) = rep(c("Index", "Value", "Cases"), 2)

pander(appendixA)
```
\newpage

# Appendix B: R Code {-}

```{r appendix-b, echo=TRUE, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
# setup -----
library(dplyr)
library(VIM)
library(ggplot2)
library(MASS)
library(gridExtra)
library(caret)
library(pROC)
library(grid)
library(leaps)
library(reshape2)
library(vcd)
library(glmnet)
library(e1071)
library(car)
library(pander)

train = read.csv("https://github.com/dsmilo/DATA621/raw/master/HW5/Data/wine-training-data.csv")
train <- train[-1] #remove index column
test = read.csv("https://github.com/dsmilo/DATA621/raw/master/HW5/Data/wine-evaluation-data.csv")

# summary -----
means <- sapply(train, function(y) mean(y, na.rm = TRUE))
mins <- sapply(train, function(y) min(y, na.rm=TRUE))
medians <- sapply(train, function(y) median(y, na.rm = TRUE))
maxs <- sapply(train, function(y) max(y, na.rm=TRUE))
IQRs <- sapply(train, function(y) IQR(y, na.rm = TRUE))
SDs <- sapply(train, function(y) sd(y, na.rm = T))
skews <- sapply(train, function(y) skewness(y, na.rm = TRUE))
cors <- as.vector(cor(train$TARGET, train[ , 1:ncol(train)], use = "complete.obs"))
NAs <- sapply(train, function(y) sum(length(which(is.na(y)))))

datasummary <- data.frame(means, mins, medians, maxs, IQRs, SDs, skews, cors, NAs)
colnames(datasummary) <- c("MEAN", "MIN","MEDIAN", "MAX", "IQR", "STD. DEV", 
                           "SKEW", "$r_{TARGET}$", "NAs")
datasummary <- round(datasummary, 2)

pander(datasummary)

# variable plots-----
cont_vars <- train %>% dplyr::select(-c(TARGET, LabelAppeal, AcidIndex, STARS))

melted <- melt(cont_vars)

ggplot(melted, aes(value)) + geom_bar(aes(fill = variable, col = variable), alpha = 0.5, show.legend = FALSE) + facet_wrap(~variable, scale="free")+ ggtitle("Distribution of Continuous Variables \n")

poisson_vars <- train %>% dplyr::select(c(TARGET, LabelAppeal, AcidIndex, STARS))

melted <- melt(poisson_vars)

ggplot(melted, aes(value)) + geom_bar(aes(fill = variable, col = variable), alpha = 0.5, show.legend = FALSE) + facet_wrap(~variable, scales="free")+ ggtitle("Distribution of Discrete Variables \n")

density_df2 <- train

melted2 <- melt(density_df2, id=1)
melted2$TARGET <- factor(melted2$TARGET)

ggplot(melted2, aes(TARGET, value)) + geom_boxplot(aes(fill = TARGET), alpha = 0.5) + facet_wrap(~variable, scales="free") + scale_fill_discrete(guide = FALSE) + scale_y_continuous('', labels = NULL, breaks = NULL) + scale_x_discrete('') + ggtitle("Distribution of Predictors by TARGET\n")

# treat NAs-----
train$STARS[is.na(train$STARS)] <- 0

ggplot(train, aes(TARGET, fill = factor(STARS))) +
  geom_histogram(binwidth=1, position="dodge") + scale_fill_discrete(name = "STARS") + theme(legend.position = "bottom")

row_has_NA <- apply(train, 1, function(x){any(is.na(x))})

sum(row_has_NA)

wine_new <- train

wine_new$ResidualSugar_NA <- factor(ifelse(is.na(train$ResidualSugar), 1, 0))
wine_new$Chlorides_NA <- factor(ifelse(is.na(train$Chlorides), 1, 0))
wine_new$FreeSulfurDioxide_NA <- factor(ifelse(is.na(train$FreeSulfurDioxide), 1, 0))
wine_new$TotalSulfurDioxide_NA <- factor(ifelse(is.na(train$TotalSulfurDioxide), 1, 0))
wine_new$pH_NA <- factor(ifelse(is.na(train$pH), 1, 0))
wine_new$Sulphates_NA <- factor(ifelse(is.na(train$Sulphates), 1, 0))
wine_new$Alcohol_NA <- factor(ifelse(is.na(train$Alcohol), 1, 0))


p1 <- ggplot(wine_new, aes(x=ResidualSugar_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[16])
p2 <- ggplot(wine_new, aes(x=Chlorides_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[17])
p3 <- ggplot(wine_new, aes(x=FreeSulfurDioxide_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[18])
p4 <- ggplot(wine_new, aes(x=TotalSulfurDioxide_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[19])
p5 <- ggplot(wine_new, aes(x=pH_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[20])
p6 <- ggplot(wine_new, aes(x=Sulphates_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[21])
p7 <- ggplot(wine_new, aes(x=Alcohol_NA, y=TARGET)) + geom_violin() + geom_jitter(alpha=0.2, size=0.2, col="skyblue") + xlab(colnames(wine_new)[22])

grid.arrange(p1,p2,p3,p4,p5,p6,p7, ncol=2)

df_varHasNA <- train %>% 
  dplyr::select(-c(TARGET, FixedAcidity, VolatileAcidity, CitricAcid,Density, LabelAppeal, AcidIndex, STARS))

aggr(df_varHasNA, prop = TRUE, numbers = TRUE, sortVars = TRUE, cex.lab = 0.4, cex.axis = par("cex"), cex.numbers = par("cex"))

train <- data.frame(train[complete.cases(train), ])

melted3 <- melt(train)

ggplot(melted3, aes(value)) + geom_bar(aes(fill = variable, col = variable), alpha = 0.5, show.legend = FALSE) + facet_wrap(~variable, scale="free")+ ggtitle("Density of Variables After Casewise Deletion\n")

# predictor selection-----
regfit.full=regsubsets(TARGET ~., data=train, nvmax = 14)
reg.summary <- summary(regfit.full)

par(mfrow=c(1,2))
plot(regfit.full, scale = "bic", main = "Predictor Variables vs. BIC")
plot(reg.summary$bic, xlab="Number of Predictors", ylab="BIC", type="l", main="Best subset Selection using BIC")
points(7, reg.summary$bic[7], col="red", cex=2, pch=20)

par(mfrow=c(1,2))
plot(regfit.full, scale="Cp", main="Predictor Variables vs. Cp")
plot(reg.summary$cp, xlab="Number of Predictors", ylab="Cp", type="l", main="Best subset Selection using Cp")
points(11, reg.summary$cp[11], col="red", cex=2, pch=20)
par(mfrow=c(1,1))

# Poisson models -----
model0 <- (glm(TARGET ~ ., family="poisson", data=train))
pander(summary(model0))

model1 <- glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + TotalSulfurDioxide + LabelAppeal + AcidIndex + STARS + Alcohol + Density + Sulphates + pH, family="poisson", data=train)
pander(summary(model1))

model2 <- (glm(TARGET ~ LabelAppeal + AcidIndex + STARS, family=poisson, data=train))
pander(summary(model2))

# NB models -----
model3 <- glm.nb(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + TotalSulfurDioxide + AcidIndex + LabelAppeal + STARS, data=train)
pander(summary(model3))

model4 <- glm.nb(TARGET ~ VolatileAcidity + TotalSulfurDioxide + LabelAppeal + AcidIndex + STARS, data=train)
pander(summary(model4))

# linear models -----
model5 <-glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + TotalSulfurDioxide + LabelAppeal + log(AcidIndex) + STARS, family="gaussian", data=train)
pander(summary(model5))

model6 <- glm(TARGET ~ VolatileAcidity + FreeSulfurDioxide + TotalSulfurDioxide + Chlorides + Density + pH + Sulphates + LabelAppeal + AcidIndex + STARS, family=gaussian, data = train)
pander(summary(model6))

# cross validation-----
k = 10
set.seed(1306)
folds = sample(1:k, nrow(train), replace = TRUE)

cv.errors0 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors1 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors2 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors3 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors4 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors5 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors6 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))

#train$target = as.numeric(as.character(train$target))

for (j in 1:k) {
  model0 <- glm(TARGET ~ ., family="poisson", data=train[folds != j, ])
  model1 <- glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + 
                  TotalSulfurDioxide + LabelAppeal + AcidIndex + STARS + Alcohol + 
                  Density + Sulphates + pH, family="poisson", data=train[folds != j, ])
  model2 <- glm(TARGET ~ LabelAppeal + AcidIndex + STARS, family="poisson", data=train[folds != j, ])
  model3 <- glm.nb(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + 
                      TotalSulfurDioxide + Alcohol + AcidIndex + LabelAppeal + STARS, data=train[folds != j, ])
  model4 <- glm.nb(TARGET ~ VolatileAcidity + TotalSulfurDioxide + LabelAppeal +
                      AcidIndex + STARS, data=train[folds != j, ])
  model5 <- glm(TARGET ~ VolatileAcidity + Chlorides + FreeSulfurDioxide + 
                  TotalSulfurDioxide + LabelAppeal + log(AcidIndex) + STARS, family="gaussian", data=train[folds != j, ])
  model6 <- glm(TARGET ~ VolatileAcidity + FreeSulfurDioxide + TotalSulfurDioxide + 
                  Chlorides + Density + pH + Sulphates + LabelAppeal + AcidIndex + STARS, family="gaussian", data = train[folds != j, ])
  
  
    #best.fit = regsubsets(y ~ ., data = train_df[folds != j, ], nvmax = 10)
    for (i in 1:10) {
      f = train[folds == j, ]
      #f = f[complete.cases(f),]
      
      pred0 = predict(model0, f, id = i)
      cv.errors0[j, i] = mean((train$TARGET[folds == j] - pred0) ^ 2, na.rm = TRUE)
      
      pred1 = predict(model1, f, id = i)
      cv.errors1[j, i] = mean((train$TARGET[folds == j] - pred1) ^ 2, na.rm = TRUE)

      pred2 = predict(model2, f, id = i) 
      cv.errors2[j, i] = mean((train$TARGET[folds == j] - pred2) ^ 2, na.rm = TRUE)

      pred3 = predict(model3, f, id = i) 
      cv.errors3[j, i] = mean((train$TARGET[folds == j] - pred3) ^ 2, na.rm = TRUE)
       
      pred4 = predict(model4, f, id = i) 
      cv.errors4[j, i] = mean((train$TARGET[folds == j] - pred4) ^ 2, na.rm = TRUE)
       
      pred5 = predict(model5, f, id = i) 
      cv.errors5[j, i] = mean((train$TARGET[folds == j] - pred5) ^ 2, na.rm = TRUE)      
       
      pred6 = predict(model6, f, id = i)
      cv.errors6[j, i] = mean((train$TARGET[folds == j] - pred6) ^ 2, na.rm = TRUE)
      
  }
  
}

mean.cv.errors0 <- apply(cv.errors0, 2, mean)
mean.cv.errors1 <- apply(cv.errors1, 2, mean)
mean.cv.errors2 <- apply(cv.errors2, 2, mean)
mean.cv.errors3 <- apply(cv.errors3, 2, mean)
mean.cv.errors4 <- apply(cv.errors4, 2, mean)
mean.cv.errors5 <- apply(cv.errors5, 2, mean)
mean.cv.errors6 <- apply(cv.errors6, 2, mean)

all.cv.error = data.frame(
mean(mean.cv.errors0),
mean(mean.cv.errors1),
mean(mean.cv.errors2),
mean(mean.cv.errors3),
mean(mean.cv.errors4), 
mean(mean.cv.errors5),
mean(mean.cv.errors6)
)

names(all.cv.error) = c("Poisson Model 0", "Poisson Model 1", "Poisson Model 2", "NB Model 3", "NB Model 4", "MLR Model 5", "MLR Model 6")
#all.cv.error
all.cv.error = t(all.cv.error)
names(all.cv.error) = c("Model", "Mean CV Error")

pander(all.cv.error)

# prediction-----
predicted_cases <- predict(model6, test, type='response')
predicted_cases_int = round(predicted_cases, 0)

table_test <- table(predicted_cases_int) / length(predicted_cases_int)
table_test <- c(0, table_test, 0)
names(table_test)[1] <- "0"; names(table_test)[9] <- "8"
table_train <- table(train$TARGET) / length(train$TARGET)
table_ratings <- rbind(table_test, table_train)
row.names(table_ratings) <- c("Test", "Train")
pander(table_ratings, round = 3)

# prediction results -----
appendixA = data.frame(matrix(NA, nrow = 1667, ncol = 6))
appendixA[, 1] = test$IN[1:1667]
appendixA[, 2] = predicted_cases[1:1667]
appendixA[, 3] = predicted_cases_int[1:1667]
appendixA[, 4] = test$IN[1669:3335]
appendixA[, 5] = predicted_cases[1669:3335]
appendixA[, 6] = predicted_cases_int[1669:3335]
appendixA <- rbind(appendixA, c(test$IN[1668], predicted_cases[1668], predicted_cases_int[1688], rep(NA, 3)))

names(appendixA) = rep(c("Index", "Value", "Cases"), 2)

pander(appendixA)
```
