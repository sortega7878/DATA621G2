---
title: 'Homework 1: MoneyBall'
author: "Omar Pineda, Jeffrey Littlejohn, Sergio Ortega Cruz, Chester Poon, Simon Ustoyev"
date: "September 25, 2019"
output:
  html_document: default
  pdf_document: default
---

```{r load, include=FALSE}
library(psych)
library(corrr)
library(tidyr)
library(dplyr)
library(igraph)
library(ggraph)
library(readxl)
library(caTools)
library(Metrics)
library(MASS)

td <- read.csv('moneyball-training-data.csv')
ed <- read.csv('moneyball-evaluation-data.csv')
```

### 1. Data Exploration

Our dataset includes 2,276 observations, meaning performances for professional baseball teams between the years 1871-2006. Initially, we had 15 variables that we could use to model/predict TARGET_WINS, the number of wins a team will have. The variable TEAM_BATTING_HBP only has values for 191 of our observations, and TEAM_BASERUN_CS values were missing for 772 observations. Some other variables were missing a neglible numer of values. A boxplot of the values for our variables revealed outliers in our TEAM_PITCHING_H and TEAM_PITCHING_SO variables.

We also created a correlation matrix and correlation network to assess which variables are most useful for predicting TARGET_WINS and to explore possible multicollinearity between variables. TEAM_BATTING_H is the variable most highly correlated with TARGET_WINS. We visualized this and more through a correlation network with variables positioned and clustered by their correlation to one another. Red edges indicate negative correlations while blue ones indicate positive correlations.


```{r explore, echo=FALSE, message=FALSE, warning = FALSE, error=FALSE}
#1. Data Exploration

td1 <- td[,2:17] #removes index variable from training dataset

#Summary statistics for variables
describe(td1)

#Boxplot of TARGET_WINS by each variable in order to see outliers
boxplot(td1)

#Correlation matrix for variables
correlation <- correlate(td1)
correlation

#Correlation network for variables
tidy_cors <- td1 %>% 
  correlate() %>% 
  stretch()
tidy_cors

graph_cors <- tidy_cors %>%
  filter(abs(r) > .3) %>%
  graph_from_data_frame(directed = FALSE)

ggraph(graph_cors) +
  geom_edge_link(aes(edge_alpha = abs(r), edge_width = abs(r), color = r)) +
  guides(edge_alpha = "none", edge_width = "none") +
  scale_edge_colour_gradientn(limits = c(-1, 1), colors = c("firebrick2", "dodgerblue2")) +
  geom_node_point(color = "grey", size = 2) +
  geom_node_text(aes(label = name), repel = FALSE) +
  theme_graph()
```

### 2. Data Preparation

We transformed the data by first removing the INDEX variable since it was just an identification variable. We also removed TEAM_PITCHING_H and TEAM_PITCHING_SO since they had several outlier values based on our exploratory boxplots. TEAM_BATTING_HBP only had values for 191 (8.4%) of our performance observations, so we excluded it as well. We considered filling in missing values for the TEAM_BASERUN_CS variable with its mean value since we had values for 1504 (67%) of the observations, but decided to exclude it entirely since it had a very weak correlation (0.02) with TARGET_WINS. TEAM_FIELDING_DP also had several missing values but was weakly correlated with TARGET_WINS, so we removed it. We were thus left with 11 explanatory variables to predict TARGET_WINS.

TEAM_BATTING_SO and TEAM_BASERUN_SB had a few missing values and were both somewhat correlated with TARGET_WINS, so we imputed them with the average value for each respective variable.

```{r transform, echo=FALSE, message=FALSE, warning = FALSE, error=FALSE}
#2. Data Preparation

td2 <- subset(td1, select=-c(TEAM_PITCHING_H,TEAM_PITCHING_SO, TEAM_BATTING_HBP, TEAM_BASERUN_CS, TEAM_FIELDING_DP))
meanBattingSO <- mean(td2$TEAM_BATTING_SO, na.rm = TRUE)
td2$TEAM_BATTING_SO[which(is.na(td2$TEAM_BATTING_SO))] <- meanBattingSO
meanBaserunSB <- mean(td2$TEAM_BASERUN_SB, na.rm = TRUE)
td2$TEAM_BASERUN_SB[which(is.na(td2$TEAM_BASERUN_SB))] <- meanBaserunSB
#describe(td2)
```

### 3. Build Models

We built 3 different models to predict TARGET_WINS.

Model 1:

Our first model initially included all available variables to model TARGET_WINS and it produced an adjusted R^2 value of 0.286, meaning that our predictors explain about 30% of the variance in TARGET_WINS. We found that some of the predictors were not significant, so we returned to our correlation matrix to look for signs of collinearity in these variables (TEAM_PITCHING_HR, TEAM_BATTING_SO, TEAM_BATTING_BB). TEAM_PITCHING_BB was also not significant but we kept it because it's p-vaue was approximate to our significance level p=0.09>0.05.

a. TEAM_PITCHING_HR has a correlation coefficient of 0.96 with TEAM_BATTING_HR, and out of the two we chose to keep TEAM_PITCHING_HR since it correlates more strongly with TARGET_WINS.
b. TEAM_BATTING_SO is strongly correlated with TEAM_PITCHING_HR but the former is less correlated with TARGET_WINS so we remove it from our model.
c. TEAM_BATTING_BB is strongly correlated with TEAM_FIELDING_E but it correlates more with TARGET_WINS so we remove TEAM_FIELDING_E.

After making these changes, our adjusted R^2 value becomes 0.246, and all predictors are significant except for TEAM_BATTING_2B, so we decided to remove it. Our final version of model 1 uses 7 variables with all of them being significant to predict TARGET_WINS. This model has an adjusted R^2 value of 0.246.

All predictors in this model influence wins as predicted except for TEAM_PITCHING_HR (homeruns allowed) which positively impacts wins when it was predicted that it would have a negative impact. We permit this in the model as its coefficient is 0.05 which is not substantially positive. The most impactful predictor to a team's number of wins is TEAM_BATTING_3B (triples by batters) which makes sense since players that make it to the third base after batting are very likely to score a point for their team since they would only have to run one more base.

```{r model1, echo=FALSE, message=FALSE, warning = FALSE, error=FALSE}
mod1a <- lm(TARGET_WINS ~ ., data=td2)
summary(mod1a)

correlation2 <- correlate(td2)
correlation2

mod1b <- lm(TARGET_WINS ~ . -TEAM_BATTING_HR-TEAM_BATTING_SO-TEAM_FIELDING_E, data=td2)
summary(mod1b)

mod1c <- lm(TARGET_WINS ~ . -TEAM_BATTING_HR-TEAM_BATTING_SO-TEAM_FIELDING_E-TEAM_BATTING_2B, data=td2)
summary(mod1c)

#plot(mod1c)
```

Model 2:

For our second model, we use the same variables as those in our first model and implement a square root tranformation on TARGET_WINS. This model's adjusted R^2 increases to 0.253. We then removed 132 influential points that we identified using Cook's Distances, and our resulting model's adjusted R^2 value increased to 0.3. 

The coefficients for this model tell the same story as those in our first model, but in this model, the predictors explain the variance in our wins better.

```{r model2, echo=FALSE, message=FALSE, warning = FALSE, error=FALSE}
mod2a <- lm(sqrt(TARGET_WINS) ~ . -TEAM_BATTING_HR-TEAM_BATTING_SO-TEAM_FIELDING_E-TEAM_BATTING_2B, data=td2)
summary(mod2a)

#identifying and removing influential points
sample_size = nrow(td2)
cooksd <- cooks.distance(mod2a)
influential <- as.numeric(names(cooksd)[(cooksd > (4/sample_size))])

#new model after removing influential points
td3 <- td2[-influential,]
mod2b <- lm(sqrt(TARGET_WINS) ~ . -TEAM_BATTING_HR-TEAM_BATTING_SO-TEAM_FIELDING_E-TEAM_BATTING_2B, data=td3)
summary(mod2b)

#plot(mod2a)
```

Model 3:

We used forward selection to build our last model, which starts with no predictors in the model, iteratively adds the most contributive predictors, and stops when the improvement is no longer statistically significant. It has 8 variables and an adjusted R^2 of 0.286. All predictors are significant. 

In this model, all explanatory variables behave as expected except for TEAM_BATTING_2B and TEAM_PITCHING_BB, but both of these coefficients are close enough to 0 that we can dismiss their change in sign. TEAM_BATTIG_3B has the largest influence on TARGET_WINS, similar to what we found in model 1 which makes sense as we previously discussed. It is followed by TEAM_BATTING_HR (homeruns allowed) and this follows our logic as homeruns by batters would naturally contribute to the number of wins for a team.

```{r model3, echo=FALSE, message=FALSE, warning = FALSE, error=FALSE}
mod3a1 <- lm(TARGET_WINS ~ ., data=td2)
mod3a2 <- lm(TARGET_WINS ~ 1, data=td2)
mod3a <- stepAIC(mod3a2, direction="forward", scope = list(upper=mod3a1, lower=mod3a2))
summary(mod3a)
```

### 4. Select Models

We will select our best multiple linear regression model based on its adjusted R^2 value, mean squared error, F-statistic and residual plots as summarized below. 

Model 3 has the lowest MSE and only explains 1% less of the variance in the predicted wins than model 2, the model with the highest R^2 value. The p-values associated with the F-statistic for all models are statistically significant. Model 3 has the lowest F-statistic but it is not much lower than that of the other models. The residuals for all three models appear to be normally distributed. We thus chose model 3, our forward selection model, as our best method of modeling/predicting the number of wins that a team would have.

```{r summary, echo=FALSE, message=FALSE, warning = FALSE, error=FALSE}
summary(mod1c)
summary(mod2b)
summary(mod3a)

trainMod1WINS <- predict(mod1c, td2[,-td2$TARGET_WINS])
trainMod2WINS <- predict(mod2b, td2[,-td2$TARGET_WINS])
trainMod3WINS <- predict(mod3a, td2[,-td2$TARGET_WINS])
mse1 <- mse(td2$TARGET_WINS, trainMod1WINS)
mse2 <- mse(td2$TARGET_WINS, trainMod2WINS)
mse3 <- mse(td2$TARGET_WINS, trainMod3WINS)

model <- c("Model 1", "Model 2", "Model 3")
adjR2Values <- c(0.2458,0.2995, 0.286)
mseValues <- c(mse1,mse2, mse3)
fStatValues <- c(124.6, 153.7, 114.9)

summ <- cbind.data.frame(model, adjR2Values, mseValues, fStatValues)
summ

#residual plots
plot(fitted(mod1c), residuals(mod1c), xlab = "Fitted Value", ylab = "Residual")
plot(fitted(mod2b), residuals(mod2b), xlab = "Fitted Value", ylab = "Residual")
plot(fitted(mod3a), residuals(mod3a), xlab = "Fitted Value", ylab = "Residual")
```

Finally, we also predicted wins for the performances in our evaluation dataset. We prepared the evaluation dataset in a similar way to how we prepared the training dataset, and then we predited wins using all 3 of our models. Model 2 predicted substantially fewer wins than model 1 and model 3 did, as demonstrated through a boxplot.

```{r evaluation, echo=FALSE, message=FALSE, warning = FALSE, error=FALSE}
#preparing the evaluation dataset, imputating missing values similar to what we did with the training dataset
ed1 <- ed[,2:16] #removes index variable
ed2 <- subset(ed1, select=-c(TEAM_PITCHING_H,TEAM_PITCHING_SO, TEAM_BATTING_HBP, TEAM_BASERUN_CS, TEAM_FIELDING_DP))
meanBattingSOed <- mean(ed2$TEAM_BATTING_SO, na.rm = TRUE)
ed2$TEAM_BATTING_SO[which(is.na(ed2$TEAM_BATTING_SO))] <- meanBattingSOed
meanBaserunSBed <- mean(ed2$TEAM_BASERUN_SB, na.rm = TRUE)
ed2$TEAM_BASERUN_SB[which(is.na(ed2$TEAM_BASERUN_SB))] <- meanBaserunSBed
describe(ed2)

#predicted TARGET_WINS values for our models
ed2$mod1WINS <- predict(mod1c, ed2)
ed2$mod2WINS <- predict(mod2b, ed2)
ed2$mod3WINS <- predict(mod3a, ed2)

head(ed2)
boxplot(ed2[,11:13])
```

### Appendix 
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```